%Background (a brief technical discussion of the D* Lite search method you implemented); figures may be helpful here; 


\section{Background}
	This section will contain brief technical discussion and intuition for each of the three searches.
	\subsection{Naive Re-planning A*}
	    The Naive Re-planning A* is an adaption of the classic A* Search to an environment that is not completely observable. In an A* Search, the rational agent prioritizes nodes $s$ in the fringe based on the $g(s)+h(s)$ value, which is the backwards cost of $s$ summed with the heuristic estimation for $s$. In the case of Naive Re-planning A*, the only sensor information that the rational agent has are the eight adjacent cells (we assume a grid world environment). Therefore, when the Naive Re-planning A* agent has a search-generated path, it must follow that path until a) it has reached the goal or b) it encounters an obstacle. What the Naive Re-planning A* agent does when it reaches an obstacle is simply re-run the A* search from that location, then proceed as before. This continues until the goal is reached. Notice that when this re-planning takes place, any knowledge that might have been captured about the environment does not carry over to the next search. This is addressed in the following search algorithms.

	\subsection{Lifelong Planning A*}
	    The Lifelong Planning A* search algorithm is an incremental heuristic search. It is incremental because it uses information from previous search tasks to inform the current search task. Lifelong Planning A* similarly keeps track of the $g(s),h(s)$ values for the nodes, and additionally maintains a list of $rhs(s)$ values for the nodes, which are one-step look-ahead estimates that take into consideration the predecessors $s'$ of $s$. Formally, the $rhs$ is defined as 0 if $s=s_{start}$ and $min_{s'\in pred(s) + c(s,s')}$ otherwise. The queuing strategy for fringe nodes is slightly more complex, as it uses a tuple of keys that utilize the $g(s),h(s),$ and $rhs(s)$ values. Since the $rhs(s)$ values are a ``potentially better informed'' version of the $g(s)$ values, a node is called locally inconsistent $g(s) \neq rhs(s)$. The queue is then filled with those nodes that are locally inconsistent. By making the nodes locally consistent, an optimal path will be found. 
	    The framework for Lifelong Planning A* is as follows: it attempts to compute the shortest path to the goal node. Upon discovering environmental factors which affect the edge weights between nodes, it updates those affected nodes, makes them locally inconsistent, and therefore inserts them into the queue. It continues searching, until the goal state is found. 
	
	\subsection{D* Lite}
	Lifelong Planning A* Search is an incremental search algorithm which can carry information from one search task to the next, and thus is more effective that D* Lite. However, in Lifelong Planning A*, there are so nodes that are updated, but will have no bearing on the final search path. D* Lite aims to improve on the Lifelong Planning A* search by further optimizing it such that, upon learning of new edge costs, only those nodes are updated that will have an affect on the final search path. In order to do that, the D* Lite make some variations on the Lifelong Planning formulation. In particular, the search direction in now switched, and the $g(s)$ values now represent the goal distance, not the backwards cost. Similar to Lifelong Planning, the D* also prioritizes nodes based on a tuple that is defined in terms of  $g(s),h(s),$ and $rhs(s)$. 	


Placeholder references
\cite{koenig2002d}\cite{koenig2002improved}\cite{koenig2004lifelong}\cite{simmons1995probabilistic}