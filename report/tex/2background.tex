%Background (a brief technical discussion of the D* Lite search method you implemented); figures may be helpful here; 


\section{Background}
	This section will contain a brief technical discussion and intuition for each of the three approaches.
	
	\subsection{Naive Re-planning A*}
	    The NRA* algorithm is an adaption of the classic A* Search to an environment that is partially observable. In an A* Search, a rational agent prioritizes nodes $s$ in the fringe based on a value $f(s) = g(s)+h(s)$, which corresponds to the cost to reach the current node, plus the heuristic cost to reach the goal. In NRA*, the only information that the agent can perceive are the four adjacent cells in the assumed grid-world environment. Therefore, the NRA* agent generates an optimal path, and follows it until one of two things occur: (a) it has reached the goal or (b) it encounters a previously-unobserved obstacle. In this second case, it updates the edge weights into the obstacle square, re-runs A* search from its current location, and repeats the process as before. N.B. that when this re-planning takes place, any previous route knowledge that might have been computed previously does not carry over to the next search, resulting in significant recomputation.

	\subsection{Lifelong Planning A*}
	    The LPA* search algorithm is based on an incremental heuristic search \cite{koenig2002d} \cite{koenig2002improved}. Unlike NRA*, LPA* is able to use information from previous routefinding computations to inform new searches. To do so, LPA* keeps track of $g(s), rhs(s)$ 2-tuples for each node $s$ in the graph. Intuitively, $g(s)$ is an estimate of the cost so far, and corresponds to the same value from A* search. $rhs(s)$ is a one-step look-ahead estimate that takes into consideration the predecessors $s'$ of $s$, potentially providing a better estimate than $g(s)$. More formally, the $rhs$ is defined as 0 if $s=s_{start}$ and $min_{s'\in pred(s)} g(s')+c(s',s)$ otherwise. The queuing strategy for fringe nodes is more complex than in (NR)A*, as it uses a tuple of keys that establish a dual-priority system. Since the $rhs(s)$ values are a ``potentially better informed'' version of the $g(s)$ values, a node is called locally inconsistent if $g(s) \neq rhs(s)$. When a node becomes locally inconsistent, either through initialization or replanning, it is added to this queue.
	    
	    Initially, only the start state is locally inconsistent. By making the nodes locally consistent, and therefore making sure that the $rhs$ values are precisely the same as the $g$ values, an optimal path can be found based on the currently-known edge weights. Upon discovering environmental changes which affect the edge weights between nodes, it updates those affected nodes, and makes them locally inconsistent (or in other words, adds them to the priority queue) if necessary. It then repeats the pathfinding process until the graph is once again consistent. Note that edge weights can increase (locally overconsistent) or decrease (locally underconsistent) in general, but in our example scenario, edges can only increase. This corresponds to the agent encountering an impassable barrier. Once the graph is locally consistent, the shortest path to the goal can be found by starting at the goal state, transitioning back to the predecessor through the nodes which minimize the cost, continuing until the start state is reached.  
	
		A key drawback for LPA* is that it does not allow for changing start positions. For our motivational scenario, where the agent must directly observe the updated edge weights, this can cause significant backtracking.
	
	\subsection{D* Lite}
		 D* Lite \cite{koenig2004lifelong} \cite{simmons1995probabilistic} aims to improve on this by only updating nodes on the path from the in-progress current position to the goal state. To do so, modest changes to the LPA* algorithm are required; however, the core replanning approach of LPA* is maintained.
		
		LPA* iteratively attempts to find shortest paths from the start to the goal, incorporating new edge costs as it observes them through the local consistency approach described above. In contrast, D* Lite iteratively attempts to find the shortest path from the $current$ node to the goal, also incorporating new edge costs as it observes them, but only towards the goal. There are some other differences as well. In particular, the search direction in now switched, and the $g(s)$ values now represent the goal distance, not the backwards cost. Similarly, the $rhs$ values are now forward looking, i.e. one step look-ahead with respect to successors and their respective goal distances. Similar to LPA*, D* Lite also prioritizes nodes based on a tuple that is defined in terms of  $g(s), rhs(s)$. However, the definition of this tuple differs from that of LPA* in that its priorities are effectively lower bounds on those priorities from LPA*. It also maintains a priority offset $k_m$, which allows it to bound the replanning propagation. This re-formulation, along with a less strict terminating condition allow D* Lite to have better performance than LPA*.
		
		The framework for D* Lite is as follows: it first attempts to compute the shortest path from the start to the goal node. It then takes steps along this path, and makes any changes to the edge costs and associated priority queue values when necessary. It then computes the shortest path using this new information, and repeats. The currently-optimal shortest path to the goal can be found by starting at the current start state, transitioning to the successor which minimizes the $rhs$ condition, i.e. one step look-ahead, and continuing until the goal state is reached. However, to extract the actual path taken by the agent including replanning, this information must be extracted retrospectively, by recording the steps taken.