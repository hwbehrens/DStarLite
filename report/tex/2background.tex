%Background (a brief technical discussion of the D* Lite search method you implemented); figures may be helpful here; 


\section{Background}
	This section will contain a brief technical discussion and intuition for each of the three searches.
	\subsection{Naive Re-planning A*}
	    The Naive Re-planning A* (or NRA*) is an adaption of the classic A* Search to an environment that is not completely observable. In an A* Search, the rational agent prioritizes nodes $s$ in the fringe based on the $g(s)+h(s)$ value, i.e. the $f(s)$ value, which is the backwards cost of $s$ summed with the heuristic estimation for $s$. In the case of Naive Re-planning A*, the only sensor information that the rational agent has are the eight adjacent cells (we assume a grid world environment). Therefore, when the Naive Re-planning A* agent has a search-generated path, it must follow that path until a) it has reached the goal or b) it encounters an obstacle. When the Naive Re-planning A* encounters an obstacle, it simply updates the edge weight, and re-runs the A* search from that location, then continues as before. This continues until the goal is reached. Notice that when this re-planning takes place, any knowledge that might have been captured regarding the nodes expanded or current nodes in the fringe does not carry over to the next search. This is addressed in the following search algorithms.

	\subsection{Lifelong Planning A*}
	    The Lifelong Planning A* (or LPA*) search algorithm is an incremental heuristic search
	    \cite{koenig2002d} \cite{koenig2002improved}. It is incremental because it uses information from previous search tasks to inform the current search task. Lifelong Planning A* similarly keeps track of the $g(s),h(s)$ values for the nodes, and additionally maintains a list of $rhs(s)$ values for the nodes, which are one-step look-ahead estimates that take into consideration the predecessors $s'$ of $s$. Formally, the $rhs$ is defined as 0 if $s=s_{start}$ and $min_{s'\in pred(s)} g(s')+c(s',s)$ otherwise. The queuing strategy for fringe nodes is slightly more complex, as it uses a tuple of keys that utilize the $g(s),h(s),$ and $rhs(s)$ values. Since the $rhs(s)$ values are a ``potentially better informed'' version of the $g(s)$ values, a node is called locally inconsistent if $g(s) \neq rhs(s)$. The queue is then filled with those nodes that are locally inconsistent. Note that initially only the start state is locally inconsistent. By making the nodes locally consistent, and therefore making sure that the $rhs$ values are precisely the same as the $g$ values, an optimal path can be found. 
		The framework for Lifelong Planning A* is as follows: it attempts to compute the shortest path to the goal node. Upon discovering environmental factors which affect the edge weights between nodes, it updates those affected nodes, makes them locally inconsistent (or in other words, adds them to the priority queue) if necessary. It continues searching, until the goal state is expanded, similar to A*. Finally, the shortest path to the goal can be found by starting at the goal state, transitioning back to the predecessor which minimizes the $rhs$ condition, i.e. one step look-ahead, and continue until the start state is reached.  
	
	\subsection{D* Lite}
	In Lifelong Planning A*, there are some nodes that are updated and made locally inconsistent, but will have no bearing on the final search path. D* Lite \cite{koenig2004lifelong} \cite{simmons1995probabilistic} aims to improve on the Lifelong Planning A* search by further optimizing it such that, upon learning of new edge costs, only those nodes are updated that will have an affect on the final search path. In order to do that, the D* Lite make some variations on the Lifelong Planning formulation. 
	
	One of the key differences between LPA* and D* Lite is how they search. LPA* iteratively attempts to find shortest paths from the start to the goal, incorporating new edge costs as it observes them. On the other hand, D* Lite iteratively attempts to find the shortest path from the $current$ node and the goal, also incorporating new edge costs as it observes them. There are some other differences as well. In particular, the search direction in now switched, and the $g(s)$ values now represent the goal distance, not the backwards cost. Similarly, the $rhs$ values are now forward looking, i.e. one step look-ahead with respect to successors and their respective goal distances. Similar to Lifelong Planning, the D* also prioritizes nodes based on a tuple that is defined in terms of  $g(s),h(s),$ and $rhs(s)$. However, the definition of this tuple differs from that of LPA* in that its priorities are effectively lower bounds on those priorities from LPA*. This re-formulation, along with a less strict terminating condition allow D* Lite to have a more optimized performance than LPA*.  
	
	
	The framework for D* Lite is as follows: it attempts to compute the shortest path to the goal node. It takes this first step of the path, and makes any changes to the edge costs and associated priority queue values. It then computes the shortest path using this new information, and repeats. Finally, the shortest path to the goal can be found by starting at the start state, transitioning to the successor which minimizes the $rhs$ condition, i.e. one step look-ahead, and continue until the goal state is reached. 